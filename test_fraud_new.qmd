---
title: "EDA Fraud Detection Report"
author: "Ly Amadou"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: flatly
editor: visual
---

# üí≥ Card fraud d√©tection

| L‚Äôensemble de donn√©es contient les transactions effectu√©es par carte de cr√©dit en septembre 2013 par des titulaires de cartes europ√©ens.¬† Cet ensemble de donn√©es pr√©sente les transactions qui ont eu lieu en deux jours, o√π nous avons 492 fraudes sur 284 807 transactions. L‚Äôensemble de donn√©es est tr√®s d√©s√©quilibr√©, la classe positive (fraudes) repr√©sente 0,172% de toutes les transactions.
| Il ne contient que des variables d‚Äôentr√©e num√©riques qui sont le r√©sultat d‚Äôune transformation PCA. Malheureusement, en raison de probl√®mes de confidentialit√©, nous ne pouvons pas fournir les fonctionnalit√©s d‚Äôorigine et plus d‚Äôinformations sur les donn√©es. Caract√©ristiques V1, V2, ... Les V28 sont les principaux composants obtenus avec l‚ÄôACP, les seules caract√©ristiques qui n‚Äôont pas √©t√© transform√©es avec l‚ÄôACP sont le 'Temps' et la 'Quantit√©'. La fonction ¬´ Temps ¬ª contient les secondes √©coul√©es entre chaque transaction et la premi√®re transaction dans l‚Äôensemble de donn√©es. La fonctionnalit√© 'Montant' est le montant de la transaction, cette fonctionnalit√© peut √™tre utilis√©e pour l‚Äôapprentissage sensible aux co√ªts d√©pendant de l‚Äôexemple. La caract√©ristique 'Class' est la variable de r√©ponse et elle prend la valeur 1 en cas de fraude et 0 dans le cas contraire

```{r}
#| include: false
library(data.table)
creditcard <- data.table::fread("creditcard.csv/creditcard.csv")
# View(creditcard)
```

# Packages

```{r message=FALSE, results='hide'}
#| include: false
library(ggplot2)
library(tidyverse)
library(dplyr)
library(car)
library(lubridate)

# install.packages("caret")
library(caret)
library(plotly)
library(naniar)
library(pROC)
library(ggpubr)
library(tseries)
library(GGally)
library(pander)
library(shiny)
# install.packages("shinydashboard")
library(shinydashboard)
# install.packages("randomForest")
library(randomForest)
library(DT)
library(lubridate)
library(e1071)
# install.packages("anomalize")
library(anomalize)
library(skimr)
library(isotree)
library(dbscan)
# install.packages("kableExtra")
library(kableExtra)
library(ROSE)
library(lightgbm)
```

Faisions un petit r√©sum√© des donn√©es

```{r}
head(creditcard) %>% kbl(caption = "Les 6 premi√®res valeurs") %>% kable_classic(html_font = "cambria",full_width=T) %>%row_spec(0,background = "#FFFACD") %>% column_spec(c(1,30,31),background = "#C28AB1")
```

```{r}
cat("La dimension du jeu de donn√©s est de ", dim(creditcard)[1],"lignes et de",dim(creditcard)[2],"colonnes")
```

```{r}
creditcard %>% summary() %>% kbl(caption="R√©sum√© statistique ") %>% kable_classic(html_font ="helvetica",full_width=T)
# lapply(creditcard,sd)
```

En moyenne la valeur des transactions est de 88.35\$ le maximum 2569\$ et la transaction minimal est de 0\$ chose qui n'est pas normal (sauf cas ou fait des test par exemple lorsqu'on veut enregistr√© une CB mais qu'on doit valider avec un code secret il est frequent qu'on simule une transaction nulle )

**Classe des variables**

```{r}
par(bg="#FFFACD")
sapply(creditcard,class) %>% table() %>% barplot(,main="Classes des variables") 
grid(col="#7B106D")
```

Tout les variables sont de type **numerique** a part la variable **Class** qui est de type **entier** ont peut aller plus loin en disant que c'est un facteur(en devenir)

```{r}
# install.packages("plotly")
tb <- table(creditcard$Class)
plot_ly(
  creditcard,labels=c("Transaction legitime","Transaction frauduleuse"),values=as.numeric(tb),type="pie",marker=list(colors=c("orange","blue"))
) %>% layout(
    title=list(text="Fraud √† la carte bancaires",color="white",font="serif"),
    # legend_title_font_color="yellow",
    paper_bgcolor="#FFFACD",
    plot_bgcolor='black',
    legend=list(font=list(color="lavenderblush2"))
)
```

On observe que la proportion des donn√©es sont fortement d√©s√©quilibr√© on a √† peine 0.2% de cas de fraude autrement dit la majorit√© des transactions enregistr√©s sont consid√©r√© comme l√©gitime soit dans notre 99.8% des transactions

#### Cherchons les missing-values (valeurs manquantes)

```{r}
library(naniar)
gg_miss_var(creditcard)+theme(panel.background = element_rect(fill = "#FFFACD"),plot.background = element_rect(fill="lavenderblush2"))+labs(title="R√©partition des valeurs manquantes ",y="Valeurs manquantes")
```

**Point positive** :A priori il n'y a pas de valeurs manquantes dans le datatset donc pas besoins de faire des amputations etc..

#### Autre information

```{r}

psych::describe(creditcard) %>% kbl(caption="R√©sum√© suppl√©mentaire ") %>% kable_classic(html_font="Cambria",full_width=T) %>% kable_styling(bootstrap_options = c("striped", "hover")) %>%
  # row_spec(0, bold = T, color = "black", background = "#7B106D") %>% 
    row_spec(30:31, bold = T, color = "black", background = "#FFFACD") %>% row_spec(seq(1,29,2), bold = T, color = "white", background = "#BD85AC") 

 
```

```{r}
# 
# ggpairs(creditcard)+labs(title = "Visualisation des relation bilin√©aire entre les variables")+theme(panel.background = element_rect(fill = "#FFFACD"),plot.background = element_rect(fill="lavenderblush2"))+labs(title="R√©partition des valeurs manquantes ",y="Valeurs manquantes")
```

#### Distribution des valeurs d'Amount

```{r}
par(bg="#FFFACD")
qqnorm(creditcard$Amount)
qqline(creditcard$Amount,col="red",lwd=2)


```

Les donn√©es ne suivent pas la droite d'Henry a priori les valeurs des transaction ne suivent pas une distribution normal

```{r}
ks.test(creditcard$Amount,"pnorm",mean=mean(creditcard$Amount),sd=sd(creditcard$Amount))
```

**le "Warning: ties should not be present for the one-sample Kolmogorov-Smirnov test"** est du au fait que les donn√©es poss√®de des doublons au niveau de la variables **Amount**

Ajoutons un bruit histoire que les donn√©es diff√©rents un peu sans qu'elles soit r√©ellement modifi√©

```{r}
amount <- jitter(creditcard$Amount,1e-6)
ks.test(amount,"pnorm",mean=mean(amount),sd=sd(amount))
```

***P-value \< 1%*** voir meme 5% alors on rejette H0:**Les donn√©es sont de distribution normal**

```{r}
creditcard %>%  ggplot(aes(x=Time,y=Amount,colour = factor(Class)))+geom_point(shape="*",position = "jitter",size=1)+labs(colour="Type de transaction",title = "Les differentes transaction",subtitle = " en fonction du temps d'ecart entre chaque transactions")+theme(panel.background = element_rect(fill="lavenderblush2"),plot.background = element_rect(fill="#FFFACD"),legend.background = element_rect(fill="#FFFACD"))+scale_color_manual(values = c("0" = "#0067A7", "1" = "#FF0033"))
```

```{r}
creditcard %>%  ggplot(aes(x=Time,y=Amount,fill = factor(Class)))+geom_histogram(aes(y=..density..),bins = 50)+labs(fill="Type de transaction",title = "La densit√© des montant de transactions ")+theme(panel.background = element_rect(fill="lavenderblush2"),plot.background = element_rect(fill="#FFFACD"),legend.background = element_rect(fill="#FFFACD"))+scale_fill_manual(values = c("0" = "#0067A7", "1" = "#FF0033"))

```

Ont remarque que la densit√© des transactions frauduleuses et l√©gitime sont assez similaires mais les transaction frauduleuses sont plus denses que celle des transactions l√©gitime

```{r}
# creditcard <- 
Summary <-  creditcard %>% mutate(Heure=floor(Time/3600)) %>% group_by(Heure,Class) %>% summarise(NombreTransaction=n(),
                  Maximun=max(Amount)    ,                                    TotalTrans=sum(Amount) ,
                 TotalFraud=sum(Class),
                 MinimunTrans=min(Amount),
                 EcartType=sd(Amount),
                 MoyenTans=mean(Amount)) %>% ungroup()
  
  # La fonction ungroup est utilis√©e pour s'assurer que le dataframe r√©sultant n'est plus group√© apr√®s l'op√©ration summarise

```

```{r}
Table_resume <- Summary %>% kbl(caption="Tableau des principaux observations ") %>% kable_styling(html_font = "helvetica") %>% row_spec(seq(2,94,2),background ="#B43D76") %>% row_spec(0,background = "#804AA4") 

```

```{r}

Summary %>% ggplot(aes(x=Heure,y=TotalTrans,col=factor(Class)))+geom_line()+facet_wrap(~Class,scales = "free_y")+theme(panel.background = element_rect(fill = "lavenderblush2"),
   plot.background = element_rect(fill="#FFFACD") ,legend.background = element_rect(fill="#FFFACD"))+scale_colour_manual(values  = c("0" = "#0067A7", "1" = "#FF0033"))+labs(col="Types de transaction")
```

Entre 5h et 20h ont note le fait que les transaction l√©gitime sont √©lev√© cela peut-√™tre consid√©r√© comme des heures normaux de transactions entre 10h et 12h on a une transactionTotal assez √©lev√© c'est a cette p√©riode ou en d√©tecte le plus de fraude en 1H (11 cas de fraudes sur 43 transactions )

```{r}
Summary[Summary$NombreTransaction< 500,]

Summary %>% ggplot(aes(x=Heure,y=TotalFraud,colour = factor(Class)))+
  geom_point()+geom_smooth(method = "loess",se=F)
```

-   Les points sont dispers√©es et pr√©sentent des **pics √† diff√©rents moments** (11h et 26H)

-   On observe une augmentation de la fraude √† certaines heures sp√©cifiques entre(0h et 15h) puis de (35H a 42h)

-   Certains pics d√©passent **30 transactions frauduleuses** pour certaines heures, cela est peut √™tre un signe des moments ou les fraudeurs sont les plus actifs

```{r}
gg1 <- creditcard %>% ggplot(aes(x=Class,y=Amount))+geom_boxplot(aes(color=factor(Class)))+coord_flip()

gg2 <- creditcard %>% ggplot(aes(x=Class,y=Amount))+geom_boxplot(aes(fill=factor(Class)),outliers = F)+coord_flip()

gridExtra::grid.arrange(gg1,gg2)
```

#### Test de significativit√©

Le test de **Mann-Whitney** est un test non param√©trique qui compare les distributions de deux groupes

```{r}
# creditcard$Class %>% table()
wilcox.test(creditcard$Amount~factor(creditcard$Class))
```

Puisque le **p-value est tr√®s faible (inf√©rieur a 5%)**, nous **rejetons l'hypoth√®se nulle**, ce qui signifie que les montants des transactions diff√®rent de mani√®re significative entre les transactions frauduleuses et normales. cela a une implication direct sur le fait que **Les montants des transactions sont un facteur discriminant important** entre les transactions normales et frauduleuses.

```{r}
wilcox_results <- data.frame(Variable = character(), W = numeric(), p_value = numeric())

for (i in names(creditcard %>% dplyr::select(-Class))) {
  test_result <- wilcox.test(creditcard[[i]] ~ factor(creditcard$Class)) 
  wilcox_results <- rbind(wilcox_results, data.frame(Variable = i, W = test_result$statistic, p_value = test_result$p.value))
}

# Trier les r√©sultats par ordre croissant de p-value
wilcox_results <- wilcox_results %>% arrange(p_value)

# Afficher les r√©sultats
print(wilcox_results)

# afficher les varaibles avec un p-value < 1%

Variables <- wilcox_results[wilcox_results$p_value < 0.01,] %>%pull(Variable) %>% unique()

# pull(Variable) : extrait uniquement la colonne Variable en tant que vecteur.
# unique() : √©limine les doublons.


```

Les variables ci-dessus sont celles qui ont une distribution diff√©rente pour les cas de fraude et de non fraude

Visualisant la distribution de quelques un de ces variables

```{r}
set.seed(123)
par(mfrow=c(2,2))
for (i in Variables[sample(1:26,4)]) {
  boxplot(creditcard[[i]]~creditcard$Class,ylab = i,col=c("#71A36D","#BB76BF"))
}
```

```{r}
corr_matrix <- cor(creditcard)
library(ggplot2)
library(ggcorrplot)
library(dplyr)


# Affichage de la heatmap de corr√©lation
g <- ggcorrplot(corr_matrix, 
           method = "square", 
           # type = "lower", 
           # lab = TRUE, 
           outline.color = "white",  # Contours blancs
           colors = c("black", "lightgreen", "orange")) +
  ggtitle("Credit Card Fraud Correlation Matrix")


  ggplotly(g)
```

Les variables V17,V10,V11,V7,V3,V16,V12 et V14 ont forte corr√©lation avec la variable cible (Fraude ou pas )

V5,V25,V3,V11,V15 et V3 ont une forte corr√©lation avec la variable temporelle

```{r}
Vars <-  creditcard %>% dplyr::select( V17,V10,V11,V7,V3,V16,V12, V14,Class,Time,V5,V25)

```

```{r}
#| include: false
# # Charger les packages n√©cessaires
# library(ggplot2)
# library(dplyr)
# library(gridExtra)  # Pour afficher plusieurs graphiques sur une m√™me figure
# 
# # Utiliser le jeu de donn√©es creditcard
# creditcard <- creditcard  # Assurez-vous que votre dataset est charg√© sous ce nom
# 
# # S√©parer les classes
t0 <- creditcard %>% filter(Class == 0)  # Transactions normales
t1 <- creditcard %>% filter(Class == 1)  # Transactions frauduleuses

# S√©lectionner les variables √† visualiser (exclure Class)
var_names <- colnames(creditcard)[colnames(creditcard) != "Class"]

# Cr√©er les graphiques de densit√© pour chaque variable
plots <- lapply(var_names, function(feature) {
  ggplot() +
    geom_density(data = t0, aes(x = .data[[feature]],y=..density.. ),col = "#9C0824", alpha = 0.5) +
    geom_density(data = t1, aes(x = .data[[feature]],y=..density.. ),col = "#F8A254", alpha = 0.5) +
    labs(title = feature, x = feature, y = "Density") +
    # scale_color_manual(values = c("Non-fraud" = "blue", "Fraud" = "red")) +
    theme_minimal() +
    theme(legend.position = "bottom", legend.title = element_blank(),
          plot.title = element_text(size = 3),
          axis.text = element_text(size = 3),
          legend.text = element_text(size = 3),
          axis.title = element_text(size=4))
})

# Afficher les graphiques sous forme de grille (8 lignes, 4 colonnes)
gridExtra::grid.arrange(grobs = plots, ncol = 4)

```

![](images/clipboard-3001076167.png)

------------------------------------------------------------------------

**üèÖVariables discriminantes**

-   **V4 et V11** : Ces variables pr√©sentent des distributions nettement distinctes entre les transactions frauduleuses (Class = 1) et non frauduleuses (Class = 0). Cela signifie qu'elles sont de tr√®s bons indicateurs de fraude et qu'elles peuvent √™tre utilis√©es efficacement dans un mod√®le de classification.

-   **V12, V14, V18** : Bien que la s√©paration ne soit pas aussi nette que pour V4 et V11, ces variables montrent des diff√©rences de distribution notables entre les deux classes. Elles contiennent donc une information partielle qui peut aider √† identifier des transactions suspectes.

-   **V1, V2, V3, V10** : Ces variables ont des profils distincts en fonction de la classe, ce qui indique qu'elles pourraient √™tre utiles pour caract√©riser les comportements transactionnels frauduleux.

#### ü§ê **Variables peu discriminantes**

-   **V25, V26, V28,V27,V24,V13,V15,V22** et **V6**: Ces variables ont des distributions tr√®s similaires pour les deux classes. Leur capacit√© √† diff√©rencier une transaction frauduleuse d‚Äôune transaction l√©gitime semble donc limit√©e( a priori) .

    [**Remarque**]{.underline} :Elles pourraient ne pas √™tre pertinentes pour un mod√®le de classification ou n√©cessiteraient une transformation suppl√©mentaire pour √™tre utiles

#### üìâ **Tendances g√©n√©rales des distributions**

-   La majorit√© des variables, √† quelques exceptions pr√®s (**Time** et **Amount**), pr√©sentent une distribution centr√©e autour de z√©ro pour les transactions l√©gitimes (**Class = 0**), souvent accompagn√©e d‚Äôune asym√©trie marqu√©e vers une des extr√©mit√©s.

-   En revanche, les transactions frauduleuses tendent √† suivre une distribution plus asym√©trique et d√©cal√©e, sugg√©rant que les fraudes ont des caract√©ristiques comportementales sp√©cifiques.

------------------------------------------------------------------------

```{r}
  #       ggplot()+
  # geom_density(data = t0,aes(x = V6,y=..density.. ,colour="Fraude"))+
  # geom_density(data=t1, aes(x=V6,y=..density.. ,colour="l√©gitime" ))+
  #         scale_color_manual(values = c("Fraude"="#BB76BF","l√©gitime"="#71A36D"))
```

```{r}
library(caret)
# set.seed(123)
Index <- createDataPartition(creditcard$Class ,p=0.8,list = F)
```

```{r}
train <- creditcard[Index,]
test <- creditcard[-Index,]
train <- Vars[Index,]
test <- Vars[-Index,]

y_test <- ifelse(test$Class==1,"Oui","Non")
x_test <- test %>% dplyr::select(-Class)

y_train <- ifelse(train$Class==1,"Oui","Non")
x_train<- train %>% dplyr::select(-Class)
```

```{r}
table(train$Class) %>% rbind(table(test$Class)) %>% barplot(col=c("#F8A254","#9C0824"))
legend("topright",legend = c("Test","Train"),col=c("#9C0824","#F8A254"),pch="-",lwd=3)
grid()
```

L‚Äôanalyse de fraude dans le domaine bancaire et financier repose sur la d√©tection de transactions anormales au sein d‚Äôun grand volume de transactions l√©gitimes. Cependant, un probl√®me majeur rencontr√© dans ce type de classification est le **d√©s√©quilibre des classes** : les transactions frauduleuses sont beaucoup plus rares que les transactions l√©gitimes. Ce d√©s√©quilibre pose des d√©fis pour l‚Äôentra√Ænement des mod√®les de Machine Learning, qui ont tendance √† privil√©gier la classe majoritaire et √† n√©gliger les fraudes.

Pour am√©liorer la performance des mod√®les et garantir une d√©tection efficace des fraudes, il est essentiel d‚Äôutiliser des **techniques d‚Äô√©quilibrage des classes**, qui permettent de donner plus de poids aux transactions frauduleuses dans l‚Äôapprentissage du mod√®le.

### **Pourquoi √©quilibrer les classes en d√©tection de fraude ?**

1.  **Mod√®le biais√© vers la classe majoritaire**

    -   Dans un jeu de donn√©es fortement d√©s√©quilibr√© (ex. 99% de transactions l√©gitimes et 1% de fraudes), un mod√®le na√Øf pourrait atteindre **99% d‚Äôexactitude** en pr√©disant simplement que toutes les transactions sont l√©gitimes. Cependant, il **√©chouerait totalement √† d√©tecter les fraudes**, ce qui est l‚Äôobjectif principal.

2.  **Mauvaises m√©triques d‚Äô√©valuation**

    -   Des m√©triques classiques comme la **pr√©cision (accuracy)** ne sont pas adapt√©es √† un contexte d√©s√©quilibr√©. Par exemple, une pr√©cision √©lev√©e peut cacher le fait que le mod√®le ne d√©tecte **presque aucune fraude**. Il est donc crucial d'utiliser des m√©triques comme le **Recall, le F1-score ou le NPV (Negative Predictive Value)** qui mettent davantage l‚Äôaccent sur la capacit√© √† d√©tecter les fraudes.

**G√©n√©ralisation inefficace du mod√®le**

-   Un mod√®le entra√Æn√© sur des donn√©es d√©s√©quilibr√©es risque de mal g√©n√©raliser et d‚Äô√™tre peu performant en conditions r√©elles. En √©quilibrant les classes, on force le mod√®le √† **mieux apprendre les motifs associ√©s aux transactions frauduleuses**, am√©liorant ainsi sa robustesse.

#### **Techniques de R√©√©chantillonnage (Resampling Techniques)** {style="color:red;"}

Ces m√©thodes modifient directement la distribution des classes dans le jeu de donn√©es d'entra√Ænement.

1.  **Sur-√©chantillonnage de la classe minoritaire (Oversampling)**

-   Ajout de copies ou de transactions frauduleuses synth√©tiques pour augmenter leur proportion.

-   M√©thodes courantes :

    -   **Random Oversampling** (duplication al√©atoire des fraudes).

    -   **SMOTE (Synthetic Minority Over-sampling Technique)** : g√©n√®re de nouvelles instances synth√©tiques en interpolant des transactions frauduleuses existantes.

    -   **ADASYN (Adaptive Synthetic Sampling)** : variante de SMOTE qui g√©n√®re plus d‚Äôexemples synth√©tiques dans les r√©gions o√π la fraude est la plus difficile √† d√©tecter.

2.  **Sous-√©chantillonnage de la classe majoritaire (Undersampling)**

-   R√©duction du nombre de transactions l√©gitimes pour √©quilibrer la proportion avec les fraudes.

-   M√©thodes courantes :

    -   **Random Undersampling** (√©chantillonnage al√©atoire des transactions l√©gitimes).

    -   **Cluster Centroids** : remplace un groupe de transactions l√©gitimes par leurs centro√Ødes pour conserver un √©chantillon repr√©sentatif.

    -   **NearMiss** : s√©lectionne les transactions l√©gitimes qui sont les plus proches des transactions frauduleuses.

#### **2Ô∏è‚É£ Techniques Bas√©es sur l‚ÄôAdaptation des Algorithmes**

Plut√¥t que de modifier les donn√©es, ces m√©thodes ajustent les algorithmes pour mieux g√©rer le d√©s√©quilibre.

-   **Pond√©ration des classes (Class Weighting)**

```         
-   Attribuer un **poids plus important aux transactions frauduleuses** dans la fonction de co√ªt du mod√®le (ex. dans un mod√®le de r√©gression logistique ou une for√™t al√©atoire).

-   Impl√©ment√© dans des algorithmes comme **XGBoost, CatBoost, Random Forest ,Lightgbm et la r√©gression logistiques**
```

-   **Seuils de d√©cision ajust√©s**

```         
-   Modifier le **threshold de classification** pour donner plus de chances aux fraudes d‚Äô√™tre d√©tect√©es (ex. consid√©rons une transaction comme frauduleuse d√®s que la probabilit√© d√©passe 0.3 au lieu de 0.5  qui est par d√©faut ou chercher le best theshold avec coords de pRocs).
```

-   **Mod√®les sp√©cialis√©s pour les donn√©es d√©s√©quilibr√©es**

```         
-   Utilisation d‚Äôalgorithmes comme **One-Class SVM, Isolation Forest** ou **Anomaly Detection Models**, qui ne n√©cessitent pas d‚Äô√©quilibrage explicite.Nous allons pas voir ces mod√®le car nous allons plus se focalis√© sur des mod√®le supervis√©
```

si vous voulez plus d'explications de comment marche tout ces techniques je vous invite a consulter mes projet sur [GitHub](https://github.com/LIONPANJSHIR/Machie-learning-avec-r.git)

### Matrices de confusion

La matrice de confusion est un tableau permettant d‚Äô√©valuer la **performance d'un mod√®le** de classification.

Elle compare les pr√©dictions du mod√®le aux valeur r√©elle pour mesur√© la capacit√© du mod√®le a pouvoir r√©ellement distingu√© les diff√©rente classes

$$
\hat{Y}\text{ C'est la variable pr√©dites et Y la variables observ√© }
$$

![](images/clipboard-333852010.png)

-   **Les faux n√©gatifs (FN)** sont critiques : Ils correspondent √† des fraudes non d√©tect√©es, causant des pertes financi√®res.

-   **Les faux positifs (FP)** peuvent poser probl√®me elle correspondrait aux transaction l√©gitime bloqu√©e cela peut nuire √† l'exp√©rience client.

## **Principales M√©triques Associ√©es** {#sec-principales-m√©triques-associ√©es}

√Ä partir de la matrice de confusion, plusieurs m√©triques sont utilis√©es pour √©valuer un mod√®le de d√©tection de fraude.

#### **1. Accuracy (Exactitude)**

$$ Accuracy=\frac{TP +TN}{TP+TN+FP+FN} $$‚Äã

‚Üí **Limite :** Peu pertinent en cas de d√©s√©quilibre des classes

#### **2. Precision (Pr√©cision)**

$$
Pr√©cision=\frac{TP}{TP+FP} 
$$‚Äã

‚Üí Proportion de transactions d√©tect√©es comme frauduleuses qui sont r√©ellement frauduleuses.\
‚Üí **Utile pour √©viter les faux positifs.**

#### **3. Recall (Rappel)**

$$Recall =\frac{TP}{TP+FN} $$

‚Üí Capacit√© du mod√®le √† d√©tecter toutes les fraudes r√©elles.\

#### **4. F1-Score**

$$
\text{Score F1}= 2\times\frac{Precision \times Recall}{Precision + Recall}
$$

‚Üí **Compromis entre pr√©cision et rappel**.

#### **5. Negative Predictive Value (NPV)**

$$
NPV=\frac{TN}{TN+FN}
$$‚Äã

‚Üí Proportion de transactions pr√©dites comme **non frauduleuses** qui sont r√©ellement l√©gitimes.\

Apr√®s cette br√®ve introduction plongeons dans le bain a la recherche des transactions frauduleuses1

```{r}
library(ROSE)
train_over <- ovun.sample(Class~.,train,method = "over")$data
train_under <- ovun.sample(Class~.,train,method = "under",p=0.3)$data
train_both <- ovun.sample(Class~.,train,method = "both",p=0.3)$data


par(mfrow=c(2,2),bg="#FFFACD")
barplot(table(train$Class),main="Distribution normal du train",col=c("#B5982E","#B980A7"),cex.main=0.8,font=6,font.main=6,col.main="red")
barplot(table(train_over$Class),main="Distribution du train avec Oversampling",col=c("#B5982E","#B980A7"),cex.main=0.8,font=6,font.main=6,col.main="red")
barplot(table(train_under$Class),main="Distribution du train avec UnderSampling",col=c("#B5982E","#B980A7"),cex.main=0.8,font=6,font.main=6,col.main="red")
barplot(table(train_both$Class),main="Distribution du train avec les deux methode Under+Over-Sampling",col=c("#B5982E","#B980A7"),cex.main=0.8,font=6,font.main=6,col.main="red")
```

# 3-Model de machine learning {#sec-model-de-machine-learning-}

### 3.1 Quelques fonctions utiles

```{r}
df_metr <- function(pred,actual,pred_prob,names){
  conf <- confusionMatrix(factor(pred),factor(actual))
  Auc <- auc(roc(factor(actual),pred_prob))

  return(
  t( c(conf$byClass[c("Sensitivity","F1","Specificity","Recall","Precision","Pos Pred Value","Neg Pred Value")]
,conf$overall[c("Accuracy","Kappa")])) %>% cbind(AUC=Auc) %>% as.data.frame(row.names = names))
}



custom_summary <- function(data, lev = NULL, model = NULL) {
  library(caret)
  library(MLmetrics)  # Pour F1_Score et MCC
  
  # Matrice de confusion
  cm <- confusionMatrix(data$pred, data$obs, positive = lev[2])
  
  # Calcul des m√©triques
  sensitivity <- cm$byClass["Sensitivity"]
  specificity <- cm$byClass["Specificity"]
  precision <- cm$byClass["Precision"]
  recall <- cm$byClass["Recall"]
  f1 <- F1_Score(y_true = data$obs, y_pred = data$pred, positive = lev[2])
  mcc <- MCC(y_true = data$obs, y_pred = data$pred)

  return(c(Sensitivity = sensitivity,
           Specificity = specificity,
           Precision = precision,
           Recall = recall,
           F1 = f1,
           MCC = mcc))
}

# Modifier trainControl avec cette fonction
tr_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = custom_summary
)



# customisation choix des seuils 
eval_metrics_df <- function(pred_prob, y_true ,seuils) {
  # Initialisation d'un data frame vide
  results <- data.frame()
  
  # Auc <- auc(roc(factor(y_true), pred_prob))
  # Boucle sur chaque seuil
  for (seuil in seuils) {
    # Conversion des probabilit√©s en classes ("Oui" ou "Non")
    pred_value <- ifelse(pred_prob > seuil, "Oui", "Non")
    
    # Calcul de la matrice de confusion
    cm <- confusionMatrix(factor(pred_value), factor(y_true))
    
    # Extraction du Neg Pred Value et du Kappa
    npv <- cm$byClass["Neg Pred Value"]
    kappa <- cm$overall["Kappa"]
    F1 <-  cm$byClass["F1"]
    sensibility <- cm$byClass["Sensitivity"]
    specificity <- cm$byClass["Specificity"]
    tpv <- cm$byClass["Pos Pred Value"]
  

    # Stockage des r√©sultats dans le data frame
    results <- rbind(results, data.frame(Threshold = seuil,
                                           Neg_Pred_Value = npv,
                                           Kappa = kappa,
                                           F1=F1,
                                           Sensitivity=sensibility,
                                           Specificity=specificity))
  }
  
  return(results)
}


```

### 3.2 Model-Random Forest

```{r}
grid <- expand.grid(cp=seq(0,.0050251256
,length=200))
library(MLmetrics)
# install.packages("MLmetrics")
tr_control <- trainControl(
  method="cv",
  number=5,
  classProbs = T,
  summaryFunction = multiClassSummary ,#mnlogloss #twoClassSummary
  verboseIter = T # verifier les fold vite fait histoire de voir es ce que y a pas de soucis d'encodage ou de parametre sans perdre de temps
)
y_train <- as.factor(ifelse(train$Class==1,"Oui","Non"))
x_train <- train %>% select(-Class)

model_rpart <- train(
  y=y_train,
  x=x_train,
  method="rpart",
  trControl=tr_control,
  tuneGrid=grid,
  metric="prAUC",
  # validationData=x_test,<
  weights=ifelse(y_train=="Oui",sum(y_train=="Non")/sum(y_train=="Oui"),1)

)

Y_train <- as.factor(ifelse(train_over$Class==1,"Oui","Non"))
X_train <- train_over %>% select(-Class)


model_rpart_over <- train(
  y=Y_train,
  x=X_train,
  method="rpart",
  trControl=tr_control,
  tuneGrid=grid,
  # tuneLenght=10,
  metric="prAUC",
  # weights=ifelse(y_train=="Oui",sum(y_train=="Non")/sum(y_train=="Oui"),1)
  
)

model_rpart$results %>% View()
```

**Avec les meilleurs param√®tre**

```{r}
best_tune <- model_rpart$bestTune
model_rpart <- train(
  y=y_train,
  x=x_train,
  method="rpart",
  trControl=tr_control,
  tuneGrid=best_tune,
  metric="F1"
)

best_tune_over <- model_rpart_over$bestTune

model_rpart_over <- train(
  y=Y_train,
  x=X_train,
  method="rpart",
  trControl=tr_control,
  tuneGrid=best_tune_over,
  metric="F1"
)
```

```{r}

pred_value <- predict(model_rpart,x_test)
pred_value_over <- predict(model_rpart_over,x_test)


pred_prob <- predict(model_rpart,x_test,type="prob")[,2]
pred_prob_over <- predict(model_rpart_over,x_test,type="prob")[,2]




conf_rpart <- caret::confusionMatrix(factor(pred_value),factor(y_test))
conf_rpart_over <- caret::confusionMatrix(factor(pred_value_over),factor(y_test))



conf_rpart
conf_rpart_over
```

**changement de seuil pour**

```{r}
eval_metrics_df(pred_prob_over,y_test,seq(0,1,0.01)) %>% View()

seuil1 <- 0.18#0.35
seuil2 <- .73

  
  
pred_aju <- ifelse(pred_prob > seuil1, "Oui","Non") %>%  as.factor()

pred_aju_over <- ifelse(pred_prob_over > seuil2 ,"Oui","Non") %>% as.factor()
conf_rpart <- caret::confusionMatrix(factor(pred_value),factor(y_test))

conf_rpart_over <- caret::confusionMatrix(factor(pred_value_over),factor(y_test))

conf_rpart
conf_rpart_over
```

```{r}
params_rpart <- model_rpart_over$bestTune %>% cbind(Metric=model_rpart_over$metric ,train="over" ,df_metr(pred_value_over,y_test,pred_prob_over,"rpart_over")) %>% rbind(params_rpart)

params_rpart <- model_rpart$bestTune %>% cbind(Metric=model_rpart$metric ,train="none" ,df_metr(pred_value,y_test,pred_prob,"rpart_none")) %>% rbind(params_rpart)


Metrics <- df_metr(pred_value_over,y_test,pred_prob_over,"rpart_over") %>% rbind(Metrics)

Metrics <- df_metr(pred_value,y_test,pred_prob,"rpart_none") %>% rbind(Metrics)
Metrics %>% View()


write.csv(Metrics,"metrics_rpart1")
```

#### Comparaison normal vs oversampling

```{r}

roc_rpart <- roc(y_test,pred_prob)
roc_rpart_over <- roc(y_test,pred_prob_over)

par(pty="s")
plot(
  roc_rpart, 
  col = "blue",        # Couleur de la courbe
  lwd = 3,             # √âpaisseur de ligne
  main = "Courbes ROC normal(rouge) vs Oversampling(blue)", 
  legacy.axes = TRUE,  # Axe (0,1) au lieu de (1,0)
  print.auc = TRUE,    # Afficher l'AUC
  # auc.polygon = TRUE,  # Remplir la zone sous la courbe
  auc.polygon.col = rgb(0, 0, 1, 0.2), # Remplissage semi-transparent
  grid = TRUE          # Ajouter une grille
)
lines(roc_rpart_over,col="red")
text(0.4,0.4,paste("AUC:", round(roc_rpart_over$auc,3)),col="red")
```

#### 3.2.1 Matrices de confusion rpart

```{r}
#| warning: false
library(cvms)
tab_rpart <- as.table(conf_rpart) %>% as.data.frame() 
 tab_rpart %>% plot_confusion_matrix(
                     target_col = "Reference",
                     prediction_col = "Prediction",
                     counts_col = "Freq",                               palette = "Purples",
                     class_order = c("Oui","Non"))+labs(title="Matrice de confusion des donn√©es brutes ",subtitle = "Modele rpart",caption = "Donn√©es brutes ")+theme(plot.title = element_text(family="serif",color = "purple4"))



```

#### 3.2.2 Matrices de confusion rpart OverSampling

```{r}
#|warning: false
tab_rpart <- as.table(conf_rpart_over) %>% as.data.frame() 
 tab_rpart %>% plot_confusion_matrix(
                     target_col = "Reference",
                     prediction_col = "Prediction",
                     counts_col = "Freq",                               palette = list("low"="#FA9D4F", "high"="#C02B40"),
                     class_order = c("Oui","Non"))+labs(title="Matrice de confusion des donn√©es avec OverSampling ",subtitle = "Modele rpart",caption = "Donn√©es brutes ",x="Valeur observ√©e",y="Valeurs pr√©dites")+theme(plot.title = element_text(family="serif",color = "orange"),axis.title = element_text(family="serif"))


```

#### 3.2.3 Quelques r√©sum√© des mod√®les rpart

```{r}

Metrics <- df_metr(pred_value_over,y_test,pred_prob_over,"rpart_over") %>% rbind(Metrics)
Metrics <- df_metr(pred_value,y_test,pred_prob,"rpart_none") %>% rbind(Metrics)
Metrics %>% View()
```

#### 3.2.4 Analyse des performances des modeles (Over/under fitting)

```{r}
# Pr√©diction sur l'ensemble d'entra√Ænement
pred_train <- predict(model_rpart_over, X_train, type = "prob")[,2]  # Probabilit√© classe "Oui"

# Pr√©diction sur l'ensemble de test
pred_test <- predict(model_rpart_over, x_test, type = "prob")[,2]

pred_test1 <- predict(model_rpart, x_test, type = "prob")[,2]
pred_train1<- predict(model_rpart, X_train, type = "prob")[,2]  # Probabilit√©

roc_train <- roc(Y_train,pred_train)
roc_test <- roc(y_test,pred_test)
roc_train1 <- roc(Y_train,pred_train1)
roc_test1 <- roc(y_test,pred_test1)

par(pty="s")
plot(
  roc_train, 
  col = "blue",        # Couleur de la courbe
  lwd = 3,             # √âpaisseur de ligne
  main = "Courbes ROC Train vs Test", 
  legacy.axes = TRUE,  # Axe (0,1) au lieu de (1,0)
  print.auc = TRUE,    # Afficher l'AUC
  # auc.polygon = TRUE,  # Remplir la zone sous la courbe
  auc.polygon.col = rgb(0, 0, 1, 0.2), # Remplissage semi-transparent
  grid = TRUE          # Ajouter une grille
)
lines(roc_test,col="red")
lines(roc_test1,col="lightblue")
lines(roc_train1,col="orange")

text(0.4,0.4,paste("AUC:", round(roc_test$auc,3)),col="red")
```

**Notes:**

#### 3.2.5 Feature importance rpart (Normal vs OverSampling)

```{r}
varImp(model_rpart,scale=F) %>% plot(., main="Importance des Variables - Mod√®le RPART",cex.main=0.6) 
```

```{r}
library(rpart)
rpart.plot(model_rpart$finalModel,type=5,extra=104,tweak = 1.4,main="ghj")
```

```{r}
# rpart.plot(model_rpart_over$finalModel,type=5,main="ghj",cex=0.16)
```

### 3.3 Esemble learning

#### 3.3.1 Modele xgboost

```{r}
library(xgboost)
library(dplyr)
library(pROC)
library(caret)
train_xgb <- xgb.DMatrix(x_train %>% as.matrix(),label= as.numeric(y_train=="Oui"))
test_xgb <- xgb.DMatrix(x_test %>% as.matrix(),label=as.numeric(y_test=="Oui"))


train_xgb_over <- xgb.DMatrix(train_over %>% select(-Class) %>% as.matrix(),label= as.numeric(train_over$Class))


train_xgb_both <- xgb.DMatrix(train_both %>% select(-Class) %>% as.matrix(),label= as.numeric(train_both$Class))

train_xgb_under <- xgb.DMatrix(train_under %>% select(-Class)  %>% as.matrix(),label= as.numeric(train_under$Class))


```

```{r}
params <- list(
  booster = "gbtree",
  eta = 0.08,
  gamma = 5,
  max_depth = 7,  # R√©duire la profondeur pour √©viter le sur-apprentissage
  lambda = 0.5,
  # scale_pos_weight = class_ratio,
  colsample_bytree = 0.75,  # √âchantillonner moins de variables
  subsample = 0.85,  # R√©duire le sur-apprentissage
  objective = "binary:logistic",
  eval_metric = "logloss",#error ,logloss auc
  alpha=5
  # min_child_weight=15
)


```

```{r}
# Entra√Ænement du mod√®le XGBoost
model_xgb <- xgb.train(
  params = params,
  data = train_xgb,
  nrounds = 200,  # Augmentez ce chiffre pour un meilleur apprentissage
  early_stopping_rounds = 30,  # Arr√™t anticip√© si les performances stagnent
  watchlist = list(train = train_xgb,dtest=test_xgb),
  verbose = 1 # Maintient la r√©partition des classes

)
model_xgb_over <- xgb.train(
  params = params,
  data = train_xgb_over,
  nrounds = 200,  # Augmentez ce chiffre pour un meilleur apprentissage
  early_stopping_rounds = 30,  # Arr√™t anticip√© si les performances stagnent
  watchlist = list(train = train_xgb_over,dtest=test_xgb),
  verbose = 1  # Maintient la r√©partition des classes

)



model_xgb_under <- xgb.train(
  params = params,
  data = train_xgb_under,
  nrounds = 200,  # Augmentez ce chiffre pour un meilleur apprentissage
  early_stopping_rounds = 30,  # Arr√™t anticip√© si les performances stagnent
  watchlist = list(train = train_xgb_under,dtest=test_xgb),
  verbose = 1

)


model_xgb_both <- xgb.train(
  params = params,
  data = train_xgb_both,
  nrounds = 200,  # nb iteration
  early_stopping_rounds = 30,  # Arr√™t anticip√© si les performances stagnent
  watchlist = list(train = train_xgb_both,dtest=test_xgb),
  verbose = 1
)



```

\

```{r}


 pred_prob_over <- predict(model_xgb_over,x_test %>% as.matrix())
pred_prob_under <- predict(model_xgb_under,x_test %>% as.matrix())
  pred_prob_both <- predict(model_xgb_both,x_test %>% as.matrix())
  pred_prob_none <- predict(model_xgb,x_test %>% as.matrix())
 
 
 roc_xgb_over <- roc(y_test,pred_prob_over)

 roc_xgb_none <- roc(y_test,pred_prob_none)

 roc_xgb_under <- roc(y_test,pred_prob_under)

 roc_xgb_both <- roc(y_test,pred_prob_both)
 
# 
#  k <- coords(roc_xgb,"best",ret="threshold",transpose=T)
#  d <- coords(roc_xgb)
 
 seuil_over <- coords(roc_xgb_over,"best",ret="threshold",transpose=T)
 seuil_under <- coords(roc_xgb_under,"best",ret="threshold",transpose=T)
   seuil_none <- coords(roc_xgb_none,"best",ret="threshold",transpose=T)
   seuil_both <- coords(roc_xgb_both,"best",ret="threshold",transpose=T)
   seuil_both <-.89#0.75

     seuil_none <-0.2#,0.20

     seuil_over <- .74

     seuil_under <- 0.92
   
   # eval_metrics_df(pred_prob_under,y_test,seq(0,1,0.01)) %>% View()
   
   
   
   
   pred_aju_over <- ifelse(pred_prob_over > seuil_both, "Oui","Non") %>% factor() 
pred_aju_both <- ifelse(pred_prob_both > seuil_both, "Oui","Non") %>% factor() 
pred_aju_under <- ifelse(pred_prob_under > seuil_under, "Oui","Non") %>% factor() 
pred_aju_none <- ifelse(pred_prob_none > seuil_none, "Oui","Non") %>% factor() # 0.05085070
 # meilleur comromis trouver
 # k <- 1.330053e-05
conf_xgb_over <-  confusionMatrix(pred_aju_over,factor(y_test))

conf_xgb_none <-  confusionMatrix(pred_aju_none,factor(y_test))

conf_xgb_under <-  confusionMatrix(pred_aju_under,factor(y_test))

conf_xgb_both <-  confusionMatrix(pred_aju_both,factor(y_test))


 
#Affichage des matrices de confussion 
conf_xgb_over
conf_xgb_under
conf_xgb_both
conf_xgb_none



```

```{r}
xgb.save(model_xgb_over,"model_xgb_over1")
xgb.save(model_xgb_both,"Model_xgb_both1")
xgb.save(model_xgb,"Model_xgb_none1")

```

```{r}
 par(pty="s",bg="#FFFACD")
roc_xgb_over %>% plot(col="red",lwd=2)
lines(roc_xgb_both,lwd=2,col="orange")
lines(roc_xgb_none,lwd=2,col="violet")
lines(roc_xgb_under,lwd=2,col="blue")
legend(0.8,0.4, lwd = 2, lty = 1,cex=0.5
       , col = c("orange", "violet", "blue", "red"), 
       legend = c(
         paste("Under/Over Sampling (AUC =", round(roc_xgb_both$auc, 3), ")"),
        paste("Sans √©chantillonnage (AUC =", round(roc_xgb_none$auc, 3), ")"),
         paste("UnderSampling (AUC =", round(roc_xgb_under$auc, 3), ")"),
         paste("OverSampling (AUC =", round(roc_xgb_over$auc, 3), ")")
       ))
```

```{r}
Test_xg_params <- model_xgb$params %>% t %>% as.list() %>%  cbind (df_metr(pred_aju_none,y_test,pred_prob_none,"xgb_none") %>% as.list() %>% t,Train="none",seuil=seuil_none)%>% rbind(Test_xg_params) 

Test_xg_params <- model_xgb_over$params %>% t %>% as.list() %>%  cbind (df_metr(pred_aju_over,y_test,pred_prob_over,"xgb_over") %>% as.list() %>% t,Train="over",seuil=seuil_over)%>% rbind(Test_xg_params) 

Test_xg_params <- model_xgb_both$params %>% t %>% as.list() %>%  cbind (df_metr(pred_aju_both,y_test,pred_prob_both,"xgb_both") %>% as.list() %>% t,Train="both",seuil=seuil_both)%>% rbind(Test_xg_params) 

Test_xg_params <- model_xgb_under$params %>% t %>% as.list() %>%  cbind (df_metr(pred_aju_under,y_test,pred_prob_under,"xgb_under") %>% as.list() %>% t,Train="under",seuil=seuil_under)%>% rbind(Test_xg_params) 

# testtxgbb <- Test_xg_params
# testtxgbb[,"seuil"] <- .35,.35)

# Test_xg_params[["seuil"]]

View(Test_xg_params)

write.csv(Test_xg_params,"xgb_params.csv")


```

```{r}
Metrics <- df_metr(pred_aju_over,y_test,pred_prob_over,"xgb_over") %>% rbind(Metrics)
Metrics <- df_metr(pred_aju_none,y_test,pred_prob_none,"xgb_none") %>% rbind(Metrics)
Metrics <- df_metr(pred_aju_both,y_test,pred_prob_both,"xgb_both") %>% rbind(Metrics)
# Metrics <- df_metr(pred_aju_over,y_test,pred_prob_over,"xgb_over") %>% rbind(Metrics)
View(Metrics)

write.csv(Metrics,"metric1.csv")
```

lorsque les classes sont d√©s√©quilibr√© la courbe ROC peut etre trop \<\<optimiste\>\>,Dans ce cas il est plus efficaces d'observer la courbe **(Recall-precsion)**

#### Features importances

```{r}
importances_over <- xgb.importance(model=model_xgb_over)
```

```{r}
dtrain <- train_over %>% select(-Class) %>% as.matrix()
x2 <- shapviz(model_xgb_over, X_pred = dtrain, X = train_over, interactions = TRUE)
sv_dependence(x2, "V14", interactions = TRUE)
sv_dependence(x2, "V14", color_var = "auto", interactions = TRUE)
sv_interaction(x2)
```

```{r}
library(shapviz)
library(SHAPforxgboost)

model_xgb_over <- xgboost::xgb.load("model_xgb_over")
shap <- shap.prep(model_xgb_over,X_train=train_over %>% select(-Class) %>% as.matrix())
# P <- shap.plot.summary(shap)
shap.plot.summary(shap)
```

```{r}
library(shapviz)
x2 <- shapviz(model_xgb_over, X_pred = dtrain, X = x_train)
sv_importance(x2)  # Summary Plot

```

üßêA partir de cette partie nous alllons essayer juste de nous **focalis√©** sur **la methode oversampling** et le model avec les donn√©es **brutes**

### ¬†Lightgbm¬†

```{r}
library(lightgbm)
train_lgb <- lgb.Dataset(x_train %>% as.matrix(),label= as.numeric(y_train=="Oui"),free_raw_data = FALSE)
test_lgb <- lgb.Dataset(x_test %>% as.matrix(),label=as.numeric(y_test=="Oui"),free_raw_data = FALSE)


train_lgb_over <- lgb.Dataset(train_over %>% select(-Class) %>% as.matrix(),label= as.numeric(train_over$Class),free_raw_data = F)

train_xgb_both <- lgb.Dataset(train_both %>% as.matrix(),label= as.numeric(train_both$Class),free_raw_data = FALSE)

train_lgb_under <- lgb.Dataset(train_under %>% as.matrix(),label= as.numeric(train_under$Class),free_raw_data = FALSE)


```

```{r}
parames <- list(
  objective = "binary",
  metric = "binary_logloss",  
  max_depth = 6,
  num_leaves = 70,
  learning_rate = 0.12,
  lambda_l1 = 3,
  lambda_l2 = 1,
  min_data_in_leaf = 300,
  bagging_fraction = 0.85,  # Moins d'√©chantillons al√©atoires (85% des donn√©es utilis√©es √† chaque bagging)
  bagging_freq = 10,  # Application du bagging tous les 10 arbres
  feature_fraction = 0.85  # Utilisation de 85% des features √† chaque it√©ration
)

# rid_params <- expand.grid(
#   learning_rate = c(0.01, 0.05, 0.1),
#   num_leaves = c(31, 100),
#   max_depth = c(-1, 10, 20),
#   bagging_fraction = c(0.7, 0.8),
#   feature_fraction = c(0.7, 0.8),
#   lambda_l1 = c(0.1, 0.5, 1.0),
#   lambda_l2 = c(0.1, 0.5, 1.0)
# )

model_lgb <- lgb.train(
  params = parames,
  data=train_lgb,
  # label=train_label,
  nrounds = 100,
  valids = list(train=train_lgb,test= test_lgb),
  early_stopping_rounds = 50,
  # early_stopping=50,
  verbose = 1
)


model_lgb_over <- lgb.train(
  params = parames,
  data=train_lgb_over,
  # label=train_label,
  nrounds = 1000,
  valids = list(train=train_lgb_over,test= test_lgb),
  early_stopping_rounds = 50,
  verbose = 1
)

```

\

```{r}

pred_prob_over <- predict(model_lgb_over,x_test %>% as.matrix())
pred_prob_none <- predict(model_lgb,x_test %>% as.matrix())

# Pr√©diction des probabilit√©s sur le jeu de test
# pred_prob <- predict(model_lgb_rose, mat_test_rose)

# Calcul de la courbe ROC
roc_lgb_over <- roc(y_test, pred_prob_over)
roc_lgb_none<- roc(y_test, pred_prob_none)




# Trouver le seuil optimal pour maximiser une m√©trique (ici : sp√©cificit√©)
seuil_over <- coords(roc_lgb_over, "best", ret = "threshold", transpose = TRUE,best.method="youden")

seuil_none <- coords(roc_lgb_none, "best", ret = "threshold", transpose = TRUE,best.method="youden")

# seuil <- 0.656
# 
# res <- eval_metrics_df(pred_prob_none,y_test,seuils=seq(0,1,by=0.01))
# res1 <- eval_metrics_df(pred_prob_over,y_test,seuils=seq(0,1,by=0.01))
# View(res)
seuil_over <- 0.947
seuil_none <- .95

# Transformer ls probabilit√©s en classes avec le seuil optimal
pred_value_over <- ifelse(pred_prob_over > seuil_over, "Oui","Non")
pred_value_none <- ifelse(pred_prob_none > seuil_none, "Oui","Non")
# G√©n√©rer une matrice de confusion
conf_lgb_over <- confusionMatrix(factor(pred_value_over), factor(y_test))
# conf_matrix$byClass
conf_lgb_none <- confusionMatrix(factor(pred_value_none), factor(y_test))

conf_lgb_none
conf_lgb_over


# saveRDS(model_lgb,"model_lgb_non_avec_vars_seuil_095.rds")
# saveRDS(model_lgb_over,"model_lgb_over_avec_vars_seuil_0947_best.rds")
# 
# # view(d)
```

```{r}


```

#### Courbe d'apprentissage lgbm

```{r}
evals <- model_lgb_over$record_evals
Evals <- model_lgb$record_evals
data <- data.frame(
  iteration=1:length(evals$train$binary_logloss$eval),
  train_log=unlist(evals$train$binary_logloss$eval),
  test_log=unlist(evals$test$binary_logloss$eval)
    )

Data <- data.frame(
  iterations=1:length(Evals$train$binary_logloss$eval),
   Train_log=unlist(Evals$train$binary_logloss$eval),
  Test_log=unlist(Evals$test$binary_logloss$eval)
)

library(cowplot)

g1 <- data %>% ggplot(aes(x=iteration))+
  geom_line(aes(y=train_log,color="Train"),linewidth=0.9)+
  geom_line(aes(y=test_log,color="Test"),linewidth=0.9)+
  labs(title="Courbe d'apprentissage -lgbm",
       y="Log_loss",color=" ")+
  scale_color_manual(values = c("Train"="red","Test"="blue"))+
  theme_stata()

g2 <- Data %>% ggplot(aes(x=iterations))+
  geom_line(aes(y=Train_log,color="Train"),linewidth=0.9)+
  geom_line(aes(y=Test_log,color="Test"),linewidth=0.9)+
  labs(title="Courbe d'apprentissage -lgbm",
       y="Log_loss",color=" ")+
  scale_color_manual(values = c("Train"="red","Test"="blue"))+
  theme_stata()


plot_grid(g1,g2,labels=c("OverSampling","Normal"),label_size = 8,label_fontfamily = "serif",label_colour = "red")
  
```

On note que la courbe d'apprentissage du test et du train sont a peu pr√®s identiques et

```{r}
```

```{r}


```

```{r}
# Visualiser la courbe ROC
par(pty="s")
plot(roc_lgb_over, main = "Courbe ROC - LightGBM",col="blue")
lines(roc_lgb_none,lwd=2,col="red")
legend(0.6,0.4,lwd=2,lty=1,cex=0.5,
       col=c("red","blue"),
       legend =c(
          paste("Normal (AUC =", round(roc_lgb_none$auc, 3), ")"),
         paste("OverSampling (AUC =", round(roc_lgb_over$auc, 3), ")")))
```

```{r}


Test_lgbm_params2 <- model_lgb_over$params %>% t %>% as.list() %>%  cbind (df_metr(pred_value_over,y_test,pred_prob_over,"lgbm_over_vars") %>% as.list() %>% t,Train="over",seuil=seuil_over)  %>% rbind(Test_lgbm_params2) 

Test_lgbm_params2 <- model_lgb $params %>% t %>% as.list() %>%  cbind (df_metr(pred_value_none,y_test,pred_prob_none,"lgbm_none_vars") %>% as.list() %>% t,Train="none",seuil=seuil_none)  %>% rbind(Test_lgbm_params2) 

view(Test_lgbm_params2)

# 
# 
Metrics <- df_metr(pred_value_over,y_test,pred_prob_over,"lgb_over_vars") %>% rbind(Metrics)
# 
Metrics <- df_metr(pred_value_none,y_test,pred_prob_none,"lgb_none_vars") %>% rbind(Metrics)

View(Metrics)


write.csv(Test_lgbm_params2,"params_lgbm.csv")
write.csv(Metrics,"metric1.csv")

```

## Features Importances lgbm

```{r}
par(bg="#FFFACD")
lgb.plot.importance(lgb.importance(model_lgb_over )) 
grid()
lgb.plot.importance(lgb.importance(model_lgb )) 
grid()
```

#### Analyses suppl√©mentaire

```{r}

# TEST <- test
# TEST$pred <- pred_value_none
# TEST$pred_over <- pred_value_over
# TEST <- TEST %>% mutate(Classification = case_when(
#   Class==0 & pred=="Oui" ~"Faux_Positive",
#   Class==1 & pred=="Non" ~"Faux_negative",
#   Class==1 & pred=="Oui"~"Vrai_positive",
#   Class==0 & pred=="Non" ~ "Vrai_negative",
#   TRUE~"Bien class√©"
# ))
# 
# 
# library(ggplot2)
# 
TEST  %>%
  ggplot(aes(x =Time , y= V14,color = Classification)) +
  geom_point() +
  theme_minimal() +
  labs(title = "R√©partition des erreurs et classifications",
       x = "Type de Classification",
       y = "Nombre d'observations")

```

1.  **SHAPE**

```{r}
library(iml)
Predictor$new(model_lgb,data=x_train,y=as.factor(as.numeric(y_train=="Oui")),type = "prob")
```

```{r}
library(shapviz)

X2 <- shapviz(model_lgb,X_pred = x_train %>% as.matrix(),X=train,interactions=TRUE)
sv_dependence(X2, "V14", interactions = TRUE)
sv_dependence(X2, "V14", color_var = "auto", interactions = TRUE)
sv_interaction(x2)
```

```{r}
library(SHAPforxgboost)
shap <- shap.prep(model_lgb_over,X_train  =data.matrix(x_train))
```

# Catboost

```{r}

library(catboost)


train_cat <- catboost.load_pool(x_train %>% as.matrix(),label= as.numeric(y_train=="Oui"))
test_cat <- catboost.load_pool(x_test,label=as.numeric(y_test=="Oui"))
train_cat_over <- catboost.load_pool(train_over %>% select(-Class) %>% as.matrix(),label= train_over$Class)
# rm(train_cat_train)
```

```{r}
params <- list(
  loss_function = "Logloss",  #BalancedErrorRate,CrossEntropy,LogLoss,Recall,Kappa,WKappa
  eval_metric = "MCC",  #AUC,PRAUC,F1,MCC
  iterations = 300,  
  learning_rate = 0.024,  
  depth = 4,  
  task_type = "CPU",  
  l2_leaf_reg = 4,
  # class_weights = c(1, sum(train$Class == 0) / sum(train$Class == 1)),
  random_strength = 1,  
  border_count = 59,  
  thread_count = 7,  
  bagging_temperature = 8,#6  
  early_stopping_rounds = 80 , # stoppel'entra√Ænement si pas d'am√©lioration apr√®s 50 it√©rations
  subsample=0.8,
  colsample_bylevel=0.8
)

# Entra√Æner le mod√®le
model_cat <- catboost.train(train_cat,
                            params = params
                            ,test_pool = test_cat)

# params$class_weights <-  c(1, sum(train_over$Class == 0) / sum(train_over$Class == 1))

model_cat_over <- catboost.train(
  train_cat_over, 
  params = params
  ,test_pool = test_cat #slm
  )
# Fa

# saveRDS(model_cat_over,"cat_test.rds")
# catboost.save_model(model_cat_over,"cat_test.cbm")
```

learn: 0.9983895 test: 0.6047736 cross entropy

```         
bestTest = 0.2875645539
```

```{r}
pred_prob <- catboost.predict(model_cat, test_cat, prediction_type = "Probability")

pred_prob_over <- catboost.predict(model_cat_over, test_cat, prediction_type = "Probability")
# √âvaluation des performances

roc_cat <- roc(y_test, pred_prob)
roc_cat_over <- roc(y_test, pred_prob_over)
# auc <- auc(roc_cat)

s <- coords(roc_cat,"best",ret="threshold",transpose=T)

# eval_metrics_df(pred_prob,y_test,seq(0,1,0.01)) %>% View()
Seuil1 <- 0.36

  Seuil2 <- 0.75

pred_value <- ifelse(pred_prob > Seuil1 ,"Oui","Non") %>% as.factor()#0.4
pred_value_over <- ifelse(pred_prob_over > Seuil2,"Oui","Non") %>% as.factor() #0.6


conf_cat <- confusionMatrix(pred_value,factor(y_test))
conf_cat_over <-  confusionMatrix(pred_value_over,factor(y_test))

par(pty="s")
plot(roc_cat,lwd=2,col="red")
lines(roc_cat_over,lwd=2,col="blue")

conf_cat
conf_cat_over
```

#### Matrices de confusion catboost

```{r}
conf_matrix_data <- as.data.frame(as.table(conf_cat_over))
# install.packages("cvms")
library(cvms)

cvms::plot_confusion_matrix(conf_matrix_data,
                            target_col = "Reference",  # Vraies classes
                      prediction_col = "Prediction",  # Pr√©dictions
                      counts_col = "Freq",palette = "Purples"
                      )



```

```{r}
Test_cat_params <- params%>% t %>% as.list() %>%  cbind (df_metr(pred_value_over,y_test,pred_prob_over,"cat_over") %>% as.list() %>% t,Train="over",Seuil=Seuil2)  %>% rbind(Test_cat_params) 

Test_cat_params <- params %>% t %>% as.list() %>%  cbind (df_metr(pred_value,y_test,pred_prob,"cat_none") %>% as.list() %>% t,Train="none",Seuil=Seuil1)  %>% rbind(Test_cat_params) 

view(Test_cat_params)


write.csv(Test_cat_params,"parmes_cat.csv")


Metrics <- cbind(...1="cat_boots", df_metr(pred_value_over,y_test,pred_prob_over,"cat_over") )%>% rbind(Metrics)


Metrics <- cbind(...1="cat_boots", df_metr(pred_value,y_test,pred_prob,"cat_none") )%>% rbind(Metrics)

# Metrics <- rbind(cbind(...1="Cat_boots",Metrics),metric1)

View(Metrics)

write.csv(Metrics,file = "Metric1.csv")

```

#### Features importances

```{r}

catboost::catboost.get_feature_importance(model_cat) 
catboost.get_object_importance(model_cat,test_cat,train_cat)

```

```{r}
# install.packages("iml")
library(iml)
```

```{r}
Shapley$new()
```

# MOdel knn

```{r}
tr_control$number <- 10
grid=expand.grid(k=seq(1,30,by=2))
model_knn <- train(y=y_train,
                   x=x_train,
                   method="knn",
                   tuneGrid=grid,
                   trControl=tr_control)
```
